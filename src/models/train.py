#!/usr/bin/env python3
"""
STARSIFTER OPTIMIZED TRAINING SCRIPT
Best performing model: Feature-pruned stacking ensemble
Performance: 88.92% F1-score (macro), 89.52% accuracy
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
import pickle
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("STARSIFTER OPTIMIZED ENSEMBLE TRAINING")
print("Feature-Pruned Stacking Model (Best Performance: 88.92% F1)")
print("=" * 80)

# Load data
print("\nðŸ“‚ Loading preprocessed data...")
X_train = pd.read_csv('data/processed/X_train_final.csv')
y_train = pd.read_csv('data/processed/y_train_final.csv').values.ravel()
X_test = pd.read_csv('data/processed/X_test_final.csv')
y_test = pd.read_csv('data/processed/y_test_final.csv').values.ravel()

print(f"   Training: {X_train.shape}")
print(f"   Test: {X_test.shape}")
print(f"   Features (before pruning): {X_train.shape[1]}")

# Remove low-value features (importance < 0.5%)
print("\nðŸ” Pruning low-value features...")
low_value_features = [
    'observation_quarters',      # 0.02% importance
    'fit_quality_score',         # 0.00% importance
    'eccentricity',              # 0.00% importance
    'high_eccentricity_flag',    # 0.00% importance
    'rocky_planet_score',        # 0.06% importance
    'impact',                    # 0.06% importance
    'prad_uncertainty_ratio',    # 0.14% importance
    'geometry_score'             # 0.39% importance
]

features_to_drop = [f for f in low_value_features if f in X_train.columns]
X_train = X_train.drop(columns=features_to_drop)
X_test = X_test.drop(columns=features_to_drop)

print(f"   Features (after pruning):  {X_train.shape[1]}")
print(f"   Removed: {len(features_to_drop)} noisy features")

# Build optimized stacking ensemble
print("\nðŸ”¨ Building optimized stacking ensemble...")

estimators = [
    ('rf', RandomForestClassifier(
        n_estimators=500,
        max_depth=20,
        max_features=0.7,
        min_samples_split=3,
        min_samples_leaf=2,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    )),
    ('xgb', XGBClassifier(
        n_estimators=600,
        max_depth=10,
        learning_rate=0.05,
        subsample=0.7,
        colsample_bytree=0.7,
        gamma=0.1,
        min_child_weight=2,
        reg_lambda=1.0,
        random_state=42,
        n_jobs=-1,
        eval_metric='mlogloss'
    )),
    ('lgbm', LGBMClassifier(
        n_estimators=400,
        max_depth=10,
        learning_rate=0.08,
        num_leaves=80,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_samples=20,
        reg_lambda=0.5,
        random_state=42,
        n_jobs=-1,
        verbose=-1
    ))
]

final_estimator = LogisticRegression(
    C=0.5,
    max_iter=1000,
    random_state=42
)

model = StackingClassifier(
    estimators=estimators,
    final_estimator=final_estimator,
    cv=5,
    n_jobs=-1
)

print("   âœ“ Random Forest: 500 trees, depth=20, balanced weights")
print("   âœ“ XGBoost: 600 trees, depth=10, L2 regularization")
print("   âœ“ LightGBM: 400 trees, depth=10, L2 regularization")
print("   âœ“ Meta-learner: Regularized Logistic Regression")

# Train
print("\nðŸ”„ Training ensemble (this may take 2-3 minutes)...")
import time
start_time = time.time()
model.fit(X_train, y_train)
training_time = time.time() - start_time

print(f"âœ“ Training completed in {training_time:.1f} seconds ({training_time/60:.1f} minutes)")

# Evaluate
print("\n" + "=" * 80)
print("EVALUATION")
print("=" * 80)

# Train performance
y_train_pred = model.predict(X_train)
train_f1 = f1_score(y_train, y_train_pred, average='macro')
train_accuracy = accuracy_score(y_train, y_train_pred)

print(f"\nðŸ“Š TRAIN SET PERFORMANCE:")
print(f"   Accuracy:  {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
print(f"   F1-Score:  {train_f1:.4f} ({train_f1*100:.2f}%)")

# Test performance
y_test_pred = model.predict(X_test)
test_f1 = f1_score(y_test, y_test_pred, average='macro')
test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted')
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"\nðŸŽ¯ TEST SET PERFORMANCE (FINAL):")
print(f"   Accuracy:            {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
print(f"   F1-Score (Macro):    {test_f1:.4f} ({test_f1*100:.2f}%)")
print(f"   F1-Score (Weighted): {test_f1_weighted:.4f} ({test_f1_weighted*100:.2f}%)")

print("\nPer-Class Performance:")
report = classification_report(y_test, y_test_pred,
                              target_names=['CONFIRMED', 'CANDIDATE', 'FALSE POSITIVE'],
                              output_dict=True)
print(classification_report(y_test, y_test_pred,
                          target_names=['CONFIRMED', 'CANDIDATE', 'FALSE POSITIVE']))

print("\nConfusion Matrix:")
print("                Predicted")
print("                CONF  CAND   FP")
cm = confusion_matrix(y_test, y_test_pred)
class_names = ['CONF', 'CAND', 'FP']
for i, row in enumerate(cm):
    print(f"Actual {class_names[i]:4s}    {row[0]:4d}  {row[1]:4d} {row[2]:4d}")

# Save model
print("\n" + "=" * 80)
print("SAVING MODEL")
print("=" * 80)

with open('models/model.pkl', 'wb') as f:
    pickle.dump(model, f)

# Save feature list
feature_list = X_test.columns.tolist()
with open('models/feature_list.json', 'w') as f:
    json.dump(feature_list, f, indent=2)

# Save results
results = {
    "model": "Optimized Stacking Ensemble",
    "version": "1.0",
    "features": X_test.shape[1],
    "features_removed": len(features_to_drop),
    "removed_features": features_to_drop,
    "training_samples": len(X_train),
    "test_samples": len(X_test),
    "training_time_seconds": float(training_time),
    "test_metrics": {
        "accuracy": float(test_accuracy),
        "f1_macro": float(test_f1),
        "f1_weighted": float(test_f1_weighted),
        "train_f1_macro": float(train_f1),
        "train_accuracy": float(train_accuracy)
    },
    "per_class_metrics": {
        "CONFIRMED": {
            "f1-score": float(report['CONFIRMED']['f1-score']),
            "precision": float(report['CONFIRMED']['precision']),
            "recall": float(report['CONFIRMED']['recall']),
            "support": int(report['CONFIRMED']['support'])
        },
        "CANDIDATE": {
            "f1-score": float(report['CANDIDATE']['f1-score']),
            "precision": float(report['CANDIDATE']['precision']),
            "recall": float(report['CANDIDATE']['recall']),
            "support": int(report['CANDIDATE']['support'])
        },
        "FALSE POSITIVE": {
            "f1-score": float(report['FALSE POSITIVE']['f1-score']),
            "precision": float(report['FALSE POSITIVE']['precision']),
            "recall": float(report['FALSE POSITIVE']['recall']),
            "support": int(report['FALSE POSITIVE']['support'])
        }
    },
    "timestamp": datetime.now().isoformat()
}

with open('reports/results.json', 'w') as f:
    json.dump(results, f, indent=2)

print("   âœ“ Saved: models/model.pkl")
print("   âœ“ Saved: models/feature_list.json")
print("   âœ“ Saved: reports/results.json")

print("\n" + "=" * 80)
print("TRAINING COMPLETE!")
print("=" * 80)
print(f"\nðŸŽ‰ Final F1-Score: {test_f1*100:.2f}%")
print(f"ðŸ“ˆ Improvement from baseline (74.29%): +{(test_f1-0.7429)*100:.2f}%")
print(f"â±ï¸  Training time: {training_time/60:.1f} minutes")
